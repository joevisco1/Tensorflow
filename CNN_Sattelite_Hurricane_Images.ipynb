{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5174,"status":"ok","timestamp":1689034874553,"user":{"displayName":"joseph visco","userId":"14159228822760035158"},"user_tz":240},"id":"jNtQdenxUnY1","outputId":"64fb7e7d-d551-4365-fddc-1fdc8416ac15"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/591.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m583.7/591.0 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n","Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Installing collected packages: typeguard, tensorflow-addons\n","Successfully installed tensorflow-addons-0.20.0 typeguard-2.13.3\n"]}],"source":["!pip install tensorflow-addons"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3443,"status":"ok","timestamp":1689182170384,"user":{"displayName":"joseph visco","userId":"14159228822760035158"},"user_tz":240},"id":"w-4gv25aYbRk"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import urllib\n","import zipfile\n","import random\n","import tensorflow as tf\n","import tensorflow.keras.layers.experimental.preprocessing as augment\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":197,"status":"ok","timestamp":1689182172482,"user":{"displayName":"joseph visco","userId":"14159228822760035158"},"user_tz":240},"id":"drL7O2tiYkan"},"outputs":[],"source":["\n","def download_and_extract_data():\n","    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/satellitehurricaneimages.zip'\n","    urllib.request.urlretrieve(url, 'satellitehurricaneimages.zip')\n","    with zipfile.ZipFile('satellitehurricaneimages.zip', 'r') as zip_ref:\n","        zip_ref.extractall()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jjDN8xeYbeY1"},"outputs":[],"source":["from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n","\n","\n","callbacks = [\n","        # EarlyStopping(\n","        #     monitor='val_accuracy',\n","        #     min_delta=1e-4,\n","        #     patience=3,\n","        #     verbose=1\n","        # )\n","      #  ,\n","        ModelCheckpoint(\n","            filepath='mymodel.h5',\n","            monitor='val_accuracy',\n","            mode='max',\n","            save_best_only=True,\n","            save_weights_only=False,\n","            verbose=1\n","        )\n","        # ,\n","\n","        # ReduceLROnPlateau(monitor=\"val_loss\",\n","        #                                         factor=0.2,  # multiply the learning rate by 0.2 (reduce by 5x)\n","        #                                         patience=2,\n","        #                                         verbose=1,  # print out when learning rate goes down\n","        #                                         min_lr=1e-7)\n","        ]\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6627,"status":"ok","timestamp":1689182193350,"user":{"displayName":"joseph visco","userId":"14159228822760035158"},"user_tz":240},"id":"TaJCG5T8qy8M","outputId":"8e1f572a-09e7-4d01-88ae-1313f8be5e57"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 10000 files belonging to 2 classes.\n","Found 2000 files belonging to 2 classes.\n"]}],"source":["\n","download_and_extract_data()\n","IMG_SIZE = (128, 128)\n","BATCH_SIZE = 64\n","\n","train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  directory='train/',\n","  image_size=IMG_SIZE,\n","  batch_size=BATCH_SIZE,\n","  label_mode = 'binary')\n","\n","val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  directory='validation/',\n","  image_size= IMG_SIZE,\n","  batch_size=BATCH_SIZE,\n","  label_mode = 'binary')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ka4hXTmV5DNh"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":228,"status":"ok","timestamp":1689182195662,"user":{"displayName":"joseph visco","userId":"14159228822760035158"},"user_tz":240},"id":"RiP707WIYllz"},"outputs":[],"source":["def preprocess(image, label):\n","    image = image/255\n","    label = label/255\n","\n","\n","\n","\n","    return image, label\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":200,"status":"ok","timestamp":1689182197608,"user":{"displayName":"joseph visco","userId":"14159228822760035158"},"user_tz":240},"id":"NgIaXs1Kq-wT","outputId":"74878d5b-1397-4e07-dcdd-fc00c4c20016"},"outputs":[{"data":{"text/plain":["(<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>,\n"," <_ParallelMapDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["train_ds = train_ds.map(\n","preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(\n","tf.data.experimental.AUTOTUNE)\n","val_ds = val_ds.map(\n","preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","train_ds, val_ds\n","\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":147,"status":"ok","timestamp":1689182207277,"user":{"displayName":"joseph visco","userId":"14159228822760035158"},"user_tz":240},"id":"Rm3GYgaOjFUl"},"outputs":[],"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","import tensorflow.keras.layers.experimental.preprocessing as augment\n","\n","# # Create a data augmentation stage with horizontal flipping, rotations, zooms\n","# data_augmentation = keras.Sequential(\n","#     [\n","#         layers.RandomFlip(\"horizontal\"),\n","#         layers.RandomRotation(0.1),\n","#         layers.RandomZoom(0.1),\n","#     ]\n","# )\n","\n","data_augmentation = tf.keras.Sequential([\n","  augment.RandomFlip(\"horizontal_and_vertical\"),\n","  augment.RandomRotation(0.2),\n","  augment.RandomZoom(height_factor=(0.2, 0.3), width_factor=(0.2, 0.3)),\n","  augment.RandomTranslation(0.3, 0.3, fill_mode='reflect', interpolation='bilinear',)\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E91G0KpAWqpc"},"outputs":[],"source":["\n","train_datagen = ImageDataGenerator(\n","    rescale=1. / 255,\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest')\n","\n","validation_datagen = ImageDataGenerator(rescale=1 / 255)\n","\n","train_generator = train_datagen.flow_from_directory(\n","    'train/',  # This is the source directory for training images\n","    target_size=(300, 300),  # All images will be resized to 150x150\n","    batch_size=1,\n","    # Since we use binary_crossentropy loss, we need binary labels\n","    class_mode='binary')\n","\n","validation_generator = validation_datagen.flow_from_directory(\n","    'validation/',  # This is the source directory for training images\n","    target_size=(300, 300),  # All images will be resized to 150x150\n","    batch_size=1,\n","    # Since we use binary_crossentropy loss, we need binary labels\n","    class_mode='binary')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AMOBO-lHGf2j"},"outputs":[],"source":["from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","callbacks = [\n","        EarlyStopping(\n","            monitor='val_accuracy',\n","            min_delta=1e-4,\n","            patience=3,\n","            verbose=1\n","        ),\n","        ModelCheckpoint(\n","            filepath='mymodel.h5',\n","            monitor='val_accuracy',\n","            mode='max',\n","            save_best_only=True,\n","            save_weights_only=False,\n","            verbose=1\n","        )\n","    ]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":208,"status":"ok","timestamp":1689036614391,"user":{"displayName":"joseph visco","userId":"14159228822760035158"},"user_tz":240},"id":"qpllLQxzHV4j","outputId":"ababe5ad-f4c2-493e-916c-c2daa21fed64"},"outputs":[{"data":{"text/plain":["(<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float64, name=None))>,\n"," <_ParallelMapDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float64, name=None))>)"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["train_ds, val_ds"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":568,"status":"error","timestamp":1689036618599,"user":{"displayName":"joseph visco","userId":"14159228822760035158"},"user_tz":240},"id":"aY0paii74w4J","outputId":"481323d1-f139-4f29-b6df-857eecdcba03"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-2ebe052d8f24>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mefn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEfficientNetB0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'efn' is not defined"]}],"source":["import urllib.request\n","import os\n","import zipfile\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras import layers\n","from tensorflow.keras import Model\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","from tensorflow.keras.optimizers import RMSprop\n","\n","\n","#base_model = efn.EfficientNetB0(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet')\n","\n","\n","\n","weights_url = \"https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n","weights_file = \"inception_v3.h5\"\n","urllib.request.urlretrieve(weights_url, weights_file)\n","\n","pre_trained_model = InceptionV3(input_shape=(128, 128, 3),\n","                                include_top=False,\n","                                weights=None)\n","\n","pre_trained_model.load_weights(weights_file)\n","\n","for layer in pre_trained_model.layers:\n","    layer.trainable = False\n","\n","# pre_trained_model.summary()\n","\n","last_layer = pre_trained_model.get_layer('mixed7')\n","print('last layer output shape: ', last_layer.output_shape)\n","last_output = last_layer.output\n","\n","# Flatten the output layer to 1 dimension\n","x = layers.Flatten()(last_output)\n","# Add a fully connected layer with 1,024 hidden units and ReLU activation\n","x = layers.Dense(1024, activation='relu')(x)\n","# Add a dropout rate of 0.2\n","x = layers.Dropout(0.2)(x)\n","# Add a final sigmoid layer for classification\n","x = layers.Dense(1, activation='sigmoid')(x)\n","\n","model = Model(pre_trained_model.input, x)\n","\n","model.compile(optimizer=tf.keras.optimizers.Adam(),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","\n","train_datagen = ImageDataGenerator(\n","    rescale=1. / 255,\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest')\n","\n","validation_datagen = ImageDataGenerator(rescale=1 / 255)\n","\n","train_generator = train_datagen.flow_from_directory(\n","    'train/',  # This is the source directory for training images\n","    target_size=(128, 128),  # All images will be resized to 150x150\n","    batch_size=64,\n","    # Since we use binary_crossentropy loss, we need binary labels\n","    class_mode='binary')\n","\n","validation_generator = validation_datagen.flow_from_directory(\n","    'validation/',  # This is the source directory for training images\n","    target_size=(128, 128),  # All images will be resized to 150x150\n","    batch_size=64,\n","    # Since we use binary_crossentropy loss, we need binary labels\n","    class_mode='binary')\n","\n","\n","\n","\n","\n","model.fit(\n","        train_generator,\n","        steps_per_epoch=8,\n","        epochs=100,\n","        verbose=1,\n","        validation_data=validation_generator,\n","        validation_steps=8,\n","        callbacks=callbacks\n","    )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iz7Cgn6ycdFO"},"outputs":[],"source":["import urllib.request\n","import os\n","import zipfile\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras import layers\n","from tensorflow.keras import Model\n","from tensorflow.keras.applications import EfficientNetB7\n","from tensorflow.keras.optimizers import RMSprop\n","\n","\n","\n","\n","\n","\n","\n","# base_model = EfficientNetB7(input_shape = (128, 128, 3), include_top = False, weights = 'imagenet', drop_connect_rate=0.4)\n","\n","# for layer in base_model.layers:\n","#     layer.trainable = False\n","\n","\n","\n","# model = tf.keras.Sequential([\n","#                                 base_model,\n","#                                 tf.keras.layers.GlobalAveragePooling2D(name=\"Layer1\"),\n","#                                 tf.keras.layers.Dropout(0.4),\n","#                                 tf.keras.layers.Dense(1, activation='softmax')\n","#     ])\n","model.compile(optimizer=tf.keras.optimizers.Adam(),loss='binary_crossentropy',metrics=['accuracy'], run_eagerly=True)\n","\n","\n","\n","# train_datagen = ImageDataGenerator(\n","#     rescale=1. / 255,\n","#     rotation_range=40,\n","#     width_shift_range=0.2,\n","#     height_shift_range=0.2,\n","#     shear_range=0.2,\n","#     zoom_range=0.2,\n","#     horizontal_flip=True,\n","#     fill_mode='nearest')\n","\n","# validation_datagen = ImageDataGenerator(rescale=1 / 255)\n","\n","# train_generator = train_datagen.flow_from_directory(\n","#     'train/',  # This is the source directory for training images\n","#     target_size=(128, 128),  # All images will be resized to 150x150\n","#     batch_size=64,\n","#     # Since we use binary_crossentropy loss, we need binary labels\n","#     class_mode='binary')\n","\n","# validation_generator = validation_datagen.flow_from_directory(\n","#     'validation/',  # This is the source directory for training images\n","#     target_size=(128, 128),  # All images will be resized to 150x150\n","#     batch_size=64,\n","#     # Since we use binary_crossentropy loss, we need binary labels\n","#     class_mode='binary')\n","\n","\n","#train_ds, val_ds\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"elapsed":214,"status":"error","timestamp":1689035991002,"user":{"displayName":"joseph visco","userId":"14159228822760035158"},"user_tz":240},"id":"7xHWr9XTV-92","outputId":"4c2999a4-1933-4861-9073-d4008c44d7c6"},"outputs":[],"source":["model.fit(\n","        train_ds,\n","        steps_per_epoch=100,\n","        epochs=10,\n","        verbose=1,\n","        validation_data=val_ds,\n","        validation_steps=8,\n","        callbacks=callbacks\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60903,"status":"ok","timestamp":1689173431083,"user":{"displayName":"joseph visco","userId":"14159228822760035158"},"user_tz":240},"id":"Wa11yw_IkwE5","outputId":"4a804fdd-10e3-4808-a23e-2eefa3ba9590"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 126, 126, 32)      896       \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 63, 63, 32)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 61, 61, 64)        18496     \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 30, 30, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 28, 28, 128)       73856     \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 14, 14, 128)      0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 12, 12, 128)       147584    \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 6, 6, 128)        0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 4608)              0         \n","                                                                 \n"," dropout (Dropout)           (None, 4608)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               2359808   \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 513       \n","                                                                 \n","=================================================================\n","Total params: 2,601,153\n","Trainable params: 2,601,153\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/200\n","157/157 [==============================] - ETA: 0s - loss: 0.0428 - binary_accuracy: 0.4988\n","Epoch 1: val_loss improved from inf to 0.01614, saving model to model_weights/hurricane-model-weights.hdf5\n","157/157 [==============================] - 18s 37ms/step - loss: 0.0428 - binary_accuracy: 0.4988 - val_loss: 0.0161 - val_binary_accuracy: 0.5000\n","Epoch 2/200\n","155/157 [============================>.] - ETA: 0s - loss: 0.0155 - binary_accuracy: 0.4997\n","Epoch 2: val_loss improved from 0.01614 to 0.01467, saving model to model_weights/hurricane-model-weights.hdf5\n","157/157 [==============================] - 5s 33ms/step - loss: 0.0155 - binary_accuracy: 0.5000 - val_loss: 0.0147 - val_binary_accuracy: 0.5000\n","Epoch 3/200\n","155/157 [============================>.] - ETA: 0s - loss: 0.0144 - binary_accuracy: 0.5003\n","Epoch 3: val_loss improved from 0.01467 to 0.01412, saving model to model_weights/hurricane-model-weights.hdf5\n","157/157 [==============================] - 5s 33ms/step - loss: 0.0144 - binary_accuracy: 0.5000 - val_loss: 0.0141 - val_binary_accuracy: 0.5000\n","Epoch 4/200\n","155/157 [============================>.] - ETA: 0s - loss: 0.0141 - binary_accuracy: 0.5006\n","Epoch 4: val_loss improved from 0.01412 to 0.01379, saving model to model_weights/hurricane-model-weights.hdf5\n","157/157 [==============================] - 5s 33ms/step - loss: 0.0141 - binary_accuracy: 0.5000 - val_loss: 0.0138 - val_binary_accuracy: 0.5000\n","Epoch 5/200\n","155/157 [============================>.] - ETA: 0s - loss: 0.0137 - binary_accuracy: 0.5003\n","Epoch 5: val_loss improved from 0.01379 to 0.01357, saving model to model_weights/hurricane-model-weights.hdf5\n","157/157 [==============================] - 5s 33ms/step - loss: 0.0137 - binary_accuracy: 0.5000 - val_loss: 0.0136 - val_binary_accuracy: 0.5000\n","Epoch 6/200\n","155/157 [============================>.] - ETA: 0s - loss: 0.0135 - binary_accuracy: 0.4996\n","Epoch 6: val_loss improved from 0.01357 to 0.01346, saving model to model_weights/hurricane-model-weights.hdf5\n","157/157 [==============================] - 5s 33ms/step - loss: 0.0135 - binary_accuracy: 0.5000 - val_loss: 0.0135 - val_binary_accuracy: 0.5000\n","Epoch 7/200\n","155/157 [============================>.] - ETA: 0s - loss: 0.0134 - binary_accuracy: 0.5009\n","Epoch 7: val_loss improved from 0.01346 to 0.01336, saving model to model_weights/hurricane-model-weights.hdf5\n","157/157 [==============================] - 5s 33ms/step - loss: 0.0135 - binary_accuracy: 0.5000 - val_loss: 0.0134 - val_binary_accuracy: 0.5000\n","Epoch 8/200\n","155/157 [============================>.] - ETA: 0s - loss: 0.0134 - binary_accuracy: 0.5001\n","Epoch 8: val_loss did not improve from 0.01336\n","157/157 [==============================] - 5s 33ms/step - loss: 0.0134 - binary_accuracy: 0.5000 - val_loss: 0.0134 - val_binary_accuracy: 0.5000\n","Epoch 9/200\n","155/157 [============================>.] - ETA: 0s - loss: 0.0133 - binary_accuracy: 0.5004Restoring model weights from the end of the best epoch: 5.\n","\n","Epoch 9: val_loss improved from 0.01336 to 0.01327, saving model to model_weights/hurricane-model-weights.hdf5\n","157/157 [==============================] - 5s 33ms/step - loss: 0.0133 - binary_accuracy: 0.5000 - val_loss: 0.0133 - val_binary_accuracy: 0.5000\n","Epoch 9: early stopping\n"]}],"source":["model = keras.Sequential([\n","    layers.Input((128,128,3)),\n","\n","    # Making the base\n","    layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), activation='relu'),\n","    layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n","    layers.Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu'),\n","    layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n","    layers.Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu'),\n","    layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n","    layers.Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu'),\n","    layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n","\n","    # Making the head\n","    layers.Flatten(),\n","    layers.Dropout(0.5),\n","    layers.Dense(512, activation='relu'),\n","    layers.Dense(1, activation='sigmoid') # used sigmoid as activation function because we are using binary labels\n","])\n","\n","OPTIMIZER = keras.optimizers.Adam(\n","    learning_rate=1e-4\n",")\n","LOSS = keras.losses.binary_crossentropy\n","model.compile(\n","    optimizer=OPTIMIZER,\n","    loss=LOSS,\n","    metrics=['binary_accuracy']\n",")\n","\n","model.summary()\n","\n","early_stopping = EarlyStopping(\n","    min_delta=1e-3,\n","    patience=4,\n","    verbose=1,\n","    restore_best_weights=True\n",")\n","\n","ckpt = ModelCheckpoint(\n","    'model_weights/hurricane-model-weights.hdf5',\n","    verbose=1,\n","    save_best_only=True,\n","    save_weights_only=True\n",")\n","\n","history = model.fit(\n","    train_ds,\n","    validation_data=(val_ds),\n","    batch_size=64,\n","    callbacks=[early_stopping, ckpt],\n","    verbose=1,\n","    epochs=200\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":502},"executionInfo":{"elapsed":1914,"status":"error","timestamp":1689173877603,"user":{"displayName":"joseph visco","userId":"14159228822760035158"},"user_tz":240},"id":"xM3MvD1vmOUR","outputId":"13a57dde-a3bc-4fd4-c2d7-1dcba8e23772"},"outputs":[],"source":["from tensorflow.keras.applications.resnet50 import ResNet50\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Input, BatchNormalization, GlobalAveragePooling2D\n","from tensorflow.keras.models import Sequential, Model\n","\n","def create_model():\n","  base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n","  # for layer in base_model.layers[:-30]:\n","  #   layer.trainable = False\n","  augmented = data_augmentation(base_model)\n","  resnet = base_model(augmented)\n","  pooling = layers.GlobalAveragePooling2D()(resnet)\n","  dropout = layers.Dropout(0.4)(pooling)\n","  outputs = Dense(len(class_types), activation=\"softmax\")(dropout)\n","  model = Model(inputs=inputs, outputs=outputs)\n","\n","  return model\n","\n","model = create_model()\n","model.summary()\n","\n","model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),\n","              metrics=['acc'])\n","\n","\n","\n","history = model.fit(train_ds,\n","                    epochs = 60,\n","                    validation_data=val_ds,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":428,"status":"error","timestamp":1689182266953,"user":{"displayName":"joseph visco","userId":"14159228822760035158"},"user_tz":240},"id":"acBaVQ7FGUPm","outputId":"8955dedd-b47a-46f9-dd40-daeaf33b2ad7"},"outputs":[],"source":["from tensorflow.keras.layers import Conv2D,MaxPool2D,Dense,Flatten,BatchNormalization,Dropout\n","\n","# Initialize a sequential model\n","model = Sequential(name=\"Base_Model\")\n","\n","\n","model.add(Conv2D(32,kernel_size =(3, 3), activation='relu',input_shape=(128,128,3)))\n","model.add(data_augmentation)\n","model.add(Conv2D(32,kernel_size =(3,3), activation='relu'))\n","model.add(MaxPool2D(pool_size=(2, 2)))\n","model.add(Conv2D(64,kernel_size =(3,3), activation='relu'))\n","model.add(Conv2D(64,kernel_size =(3,3), activation='relu'))\n","model.add(MaxPool2D(pool_size=(2, 2)))\n","model.add(Conv2D(128,kernel_size =(3,3), activation='relu'))\n","model.add(Conv2D(128,kernel_size =(3,3), activation='relu'))\n","model.add(MaxPool2D(pool_size=(2, 2)))\n","\n","model.add(Flatten())\n","\n","model.add(Dense(1024,activation='relu'))\n","model.add(Dense(1024,activation='relu'))\n","\n","model.add(Dense(1,activation = 'sigmoid'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LfSVM9EuG48Y"},"outputs":[],"source":["model.compile(optimizer=tensorflow.keras.optimizers.Adam(learning_rate = 1e-4), loss='binary_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PDn0VlHxG7XX"},"outputs":[],"source":["history_1 = model.fit(train_ds,validation_data=val_ds,epochs=10)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMMZ3udPar06uDNaF8lZebb","gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
